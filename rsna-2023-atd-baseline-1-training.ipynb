{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, we are going to fine-tune a pretrained CNN model using the CT scan images from the competition. Specifically, we are going to use a ResNet-50 model pre-trained on ImageNet data. We'll make use of the [fastai](https://docs.fast.ai) library.\n\nThis being a first baseline, we are not going to make use of segmentation data, DICOM tags, or meta data. Importantly, we are not going to consider the CT scan images as sequence, which will give more global information. Our focus here is to set up a data pipeline.","metadata":{}},{"cell_type":"markdown","source":"## Code\n\n### EDA\nWe import the necessary modules.","metadata":{}},{"cell_type":"code","source":"!pip install -qU python-gdcm pylibjpeg","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:17.310404Z","iopub.execute_input":"2023-09-18T11:51:17.311092Z","iopub.status.idle":"2023-09-18T11:51:32.412330Z","shell.execute_reply.started":"2023-09-18T11:51:17.311054Z","shell.execute_reply":"2023-09-18T11:51:32.411115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom fastai.vision.all import *\nfrom fastai.basics import *\nfrom fastai.callback.all import *\nfrom fastai.medical.imaging import *\nimport shutil\nimport pydicom\nimport cv2\nimport glob\nimport gdcm\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed\n\nSEED = 44","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-18T11:51:32.415740Z","iopub.execute_input":"2023-09-18T11:51:32.416291Z","iopub.status.idle":"2023-09-18T11:51:46.575887Z","shell.execute_reply.started":"2023-09-18T11:51:32.416261Z","shell.execute_reply":"2023-09-18T11:51:46.574934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at one of the DICOM scan images.","metadata":{}},{"cell_type":"code","source":"f_dicom = pydicom.dcmread('/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/10004/21057/1000.dcm')\nimg = f_dicom.pixel_array\n\nplt.figure(figsize=(15, 15))\nplt.imshow(img, cmap=\"gray\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:46.577089Z","iopub.execute_input":"2023-09-18T11:51:46.577823Z","iopub.status.idle":"2023-09-18T11:51:47.322936Z","shell.execute_reply.started":"2023-09-18T11:51:46.577786Z","shell.execute_reply":"2023-09-18T11:51:47.322075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's 512x512 resolution image. Let's now look at the meta-data (DICOM tags) associated with the above scan.","metadata":{}},{"cell_type":"code","source":"f_dicom","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.325434Z","iopub.execute_input":"2023-09-18T11:51:47.331051Z","iopub.status.idle":"2023-09-18T11:51:47.339976Z","shell.execute_reply.started":"2023-09-18T11:51:47.331022Z","shell.execute_reply":"2023-09-18T11:51:47.338981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The _Transfer Syntax ID_ field tells us the image is encoded using RLE Lossless compression. This means we'll probably not be able to use NVIDIA DALI for decoding speedup. The 3rd entry in the _Image Position_ list is the z-coordinate and helps us stack the 2D scans for a 3D view if needed.\n\nWe are going to work with PNG images and not DICOM images. We are going to make use of a small subset of the PNG training data provided [here](https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427427); we import this data in our Kaggle notebook using Add Data. In fact, for testing our pipeline, we're going to make use of only a small number of samples from this set.","metadata":{}},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/rsna-2023-abdominal-trauma-converting-dicom-to-png/train_images_DICOM_500/'","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.341515Z","iopub.execute_input":"2023-09-18T11:51:47.342154Z","iopub.status.idle":"2023-09-18T11:51:47.349721Z","shell.execute_reply.started":"2023-09-18T11:51:47.342122Z","shell.execute_reply":"2023-09-18T11:51:47.349101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us look at the labels provided in *train.csv*. We note that the labels are assigned to each patient and not to each CT scan image. There are many CT images associated with each patient; some of them may reflect the injury and some may not, depending upon their z-coordinate.\n\nHowever, for simplification, for each CT scan image we'll simply assign the label given to the respective patient.","metadata":{}},{"cell_type":"code","source":"patient_labels = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train.csv')\npatient_labels.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.351133Z","iopub.execute_input":"2023-09-18T11:51:47.351750Z","iopub.status.idle":"2023-09-18T11:51:47.385539Z","shell.execute_reply.started":"2023-09-18T11:51:47.351719Z","shell.execute_reply":"2023-09-18T11:51:47.384589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do a sanity check that the *healthy* and *injury* (or *low injury* and *high injury*) probabilities add up to 1 for each type of injury.","metadata":{}},{"cell_type":"code","source":"#Check data has no NANs\nprint(patient_labels.isnull().any().any())\n#Check healthy and injury labels are complementary\nprint((patient_labels['bowel_healthy'] == np.abs(1 - patient_labels['bowel_injury'])).all())\nprint((patient_labels['extravasation_healthy'] == np.abs(1 - patient_labels['extravasation_injury'])).all())\nprint((patient_labels['kidney_healthy'] == np.abs(1 - patient_labels['kidney_low'] - patient_labels['kidney_high'])).all())\nprint((patient_labels['liver_healthy'] == np.abs(1 - patient_labels['liver_low'] - patient_labels['liver_high'])).all())\nprint((patient_labels['spleen_healthy'] == np.abs(1 - patient_labels['spleen_low'] - patient_labels['spleen_high'])).all())","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.386884Z","iopub.execute_input":"2023-09-18T11:51:47.387443Z","iopub.status.idle":"2023-09-18T11:51:47.402796Z","shell.execute_reply.started":"2023-09-18T11:51:47.387411Z","shell.execute_reply":"2023-09-18T11:51:47.401880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The *any_injury* label seems a bit redundant since it can be derived from the other injury labels. It simply means whether at least one type of bowel/extravsation/liver/spleen/kidney injury is present.","metadata":{}},{"cell_type":"code","source":"#Check consistency of any_injury label with respect to other labels\nprint((patient_labels['any_injury'] == 1 - np.min(patient_labels[['bowel_healthy', 'extravasation_healthy', \n                                                          'kidney_healthy', 'liver_healthy', 'spleen_healthy']], axis = 1)).all())","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.404027Z","iopub.execute_input":"2023-09-18T11:51:47.404425Z","iopub.status.idle":"2023-09-18T11:51:47.416711Z","shell.execute_reply.started":"2023-09-18T11:51:47.404393Z","shell.execute_reply":"2023-09-18T11:51:47.415609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Data Imabalance and Correlations\n\nLet us find the percentage of samples having different forms of injuries. This will help us know the distribution of the labels and find out whether there is imbalance in positive/negative samples.","metadata":{}},{"cell_type":"code","source":"def get_pos_percent(df, label):\n    num_entries = df.shape[0]\n    return (df[label] == 1).sum()*100/num_entries\n\ninjury_labels = ['bowel_injury', 'extravasation_injury' , 'kidney_low' , 'kidney_high', 'liver_low',\n                 'liver_high', 'spleen_low', 'spleen_high']\n\n\nfor label in injury_labels:    \n    print(f'% of {label} samples = {get_pos_percent(patient_labels, label): .3f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.418206Z","iopub.execute_input":"2023-09-18T11:51:47.418600Z","iopub.status.idle":"2023-09-18T11:51:47.429304Z","shell.execute_reply.started":"2023-09-18T11:51:47.418570Z","shell.execute_reply":"2023-09-18T11:51:47.428104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed there is a lot of imbalance between number of positive and negative samples for each type of injury.\n\nNow, let us check how much the injuries are correlated among themselves. We'll plot a heatmap of the Pearson correlation coefficients (between -1 and 1).","metadata":{}},{"cell_type":"code","source":"sns.heatmap(patient_labels[injury_labels].corr(), vmin = -1, vmax = 1, annot = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.434022Z","iopub.execute_input":"2023-09-18T11:51:47.434394Z","iopub.status.idle":"2023-09-18T11:51:47.987396Z","shell.execute_reply.started":"2023-09-18T11:51:47.434276Z","shell.execute_reply":"2023-09-18T11:51:47.985880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlations are very weak, with most values close to 0 and maximum as 0.2.","metadata":{}},{"cell_type":"markdown","source":"### Data Preparation\n\n#### Splitting dataset\n\nLet us first form training and validation sets. Because of the imbalance, we would like to ensure that the class/injury distribution is similar in the three sets.","metadata":{}},{"cell_type":"code","source":"train_patients, validation_patients = train_test_split(patient_labels, test_size = 0.2, random_state = SEED) ","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.988715Z","iopub.execute_input":"2023-09-18T11:51:47.989676Z","iopub.status.idle":"2023-09-18T11:51:47.996441Z","shell.execute_reply.started":"2023-09-18T11:51:47.989643Z","shell.execute_reply":"2023-09-18T11:51:47.995436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of samples')\nprint(f'Training set: {train_patients.shape[0]}   Validation set: {validation_patients.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:47.999029Z","iopub.execute_input":"2023-09-18T11:51:47.999380Z","iopub.status.idle":"2023-09-18T11:51:48.006436Z","shell.execute_reply.started":"2023-09-18T11:51:47.999349Z","shell.execute_reply":"2023-09-18T11:51:48.005565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'% of positive samples - Training, Validation, Test')\nfor label in injury_labels:    \n    print(f'{label}: {get_pos_percent(train_patients, label): .3f}, {get_pos_percent(validation_patients, label): .3f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:48.008017Z","iopub.execute_input":"2023-09-18T11:51:48.008358Z","iopub.status.idle":"2023-09-18T11:51:48.022535Z","shell.execute_reply.started":"2023-09-18T11:51:48.008327Z","shell.execute_reply":"2023-09-18T11:51:48.021555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the labels in not exactly equal in the three sets, but it is more or less similar. For now, this will do for our purposes.","metadata":{}},{"cell_type":"markdown","source":"#### Multi-label Classification Data\n\nWe are going to consider our problem as multi-label classification. This is because a patient can have one of five different types of injuries, and the injuries can potentially co-exist.\n\nThe easiest way to prepare our training data for passing on to the fastai dataloader is to follow the convention used in this [tutorial](https://docs.fast.ai/tutorial.vision.html#multi-label-classification). We're going to use the patient labels in _train.csv_ to derive labels for each image.","metadata":{}},{"cell_type":"code","source":"filename_labels = pd.DataFrame()\nfilename_labels['fname'] = pd.Series(os.listdir(BASE_DIR))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:48.023656Z","iopub.execute_input":"2023-09-18T11:51:48.024357Z","iopub.status.idle":"2023-09-18T11:51:48.351236Z","shell.execute_reply.started":"2023-09-18T11:51:48.024326Z","shell.execute_reply":"2023-09-18T11:51:48.350265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From _train.csv_ , we construct a dictionary _patient_dict_ which has patient_id as key and the list of injuries as labels. Our target labels are whether or not a patient has one or more of bowel/extravasation/liver/spleen/kidney injuries; we derive the other probabilites from these predictions. We split the _injury_ probability equally between _low_ and _high_ for liver, spleen, and kidney.\n\nIdeally, we should use sigmoid heads for bowel/extravasation, and softmax heads for liver/spleen/kidney injuries. That is for a later iteration.","metadata":{}},{"cell_type":"code","source":"train_patients['is_valid'] = False\nvalidation_patients['is_valid'] = True\ntrain_val_df = pd.concat([train_patients, validation_patients])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:48.352575Z","iopub.execute_input":"2023-09-18T11:51:48.353437Z","iopub.status.idle":"2023-09-18T11:51:48.360671Z","shell.execute_reply.started":"2023-09-18T11:51:48.353401Z","shell.execute_reply":"2023-09-18T11:51:48.359727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_val_df['is_valid'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:48.362264Z","iopub.execute_input":"2023-09-18T11:51:48.362886Z","iopub.status.idle":"2023-09-18T11:51:48.374192Z","shell.execute_reply.started":"2023-09-18T11:51:48.362841Z","shell.execute_reply":"2023-09-18T11:51:48.373209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_dict = {}\n\ntarget_labels = ['bowel_injury', 'extravasation_injury' , 'kidney_healthy' , 'kidney_low' , 'kidney_high', 'liver_healthy', 'liver_low',\n                 'liver_high', 'spleen_healthy', 'spleen_low', 'spleen_high']\nreduced_target_labels = ['bowel_injury', 'extravasation_injury' , 'kidney_injury', 'liver_injury', 'spleen_injury']\n\n\nfor idx, patient_id in enumerate(train_val_df['patient_id']):\n    entry = train_val_df.iloc[idx][target_labels]\n    labels = [x for x in target_labels if entry[x].all() == 1] #all() because entry is a dataframe, hence entry[x] is a Series of len 1\n    #If kidney_high or kidney_low, simply replace by kidney_injury. Similarly for liver and spleen.\n    reduced_labels = set()\n    for lab in labels:\n        if 'healthy' not in lab:\n            if 'low' in lab or 'high' in lab:\n                reduced_labels.add(lab[0:lab.find('_') + 1] + 'injury')\n            else:\n                reduced_labels.add(lab)\n    is_valid = train_val_df.iloc[idx]['is_valid'] \n    patient_dict[patient_id] = (reduced_labels, is_valid)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:48.375755Z","iopub.execute_input":"2023-09-18T11:51:48.376355Z","iopub.status.idle":"2023-09-18T11:51:51.266838Z","shell.execute_reply.started":"2023-09-18T11:51:48.376271Z","shell.execute_reply":"2023-09-18T11:51:51.265898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having constructed our _patient_dict_ dictionary, we can loop through all the PNG filenames and for each look up the appropriate entry from the dictionary using _patient_id_ as the key. It was important to construct a dictionary first, so that this lookup can be fast using hash table.","metadata":{}},{"cell_type":"code","source":"label_list = []\nis_valid_list = []\n            \nfor scan_name in filename_labels['fname']:\n    patient_id = int(scan_name.split('_')[0])\n    labels = patient_dict[patient_id][0]\n    labels = ' '.join(labels)\n    label_list.append(labels)\n    \n    is_valid = patient_dict[patient_id][1]\n    is_valid_list.append(is_valid)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.268263Z","iopub.execute_input":"2023-09-18T11:51:51.268810Z","iopub.status.idle":"2023-09-18T11:51:51.293714Z","shell.execute_reply.started":"2023-09-18T11:51:51.268777Z","shell.execute_reply":"2023-09-18T11:51:51.292629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename_labels['labels'] = pd.Series(label_list)\nfilename_labels['is_valid'] = pd.Series(is_valid_list)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.295069Z","iopub.execute_input":"2023-09-18T11:51:51.295612Z","iopub.status.idle":"2023-09-18T11:51:51.307221Z","shell.execute_reply.started":"2023-09-18T11:51:51.295582Z","shell.execute_reply":"2023-09-18T11:51:51.306180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us see the different types of injuries present in our dataset.","metadata":{}},{"cell_type":"code","source":"pd.unique(filename_labels['labels'])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.308385Z","iopub.execute_input":"2023-09-18T11:51:51.308857Z","iopub.status.idle":"2023-09-18T11:51:51.316603Z","shell.execute_reply.started":"2023-09-18T11:51:51.308827Z","shell.execute_reply":"2023-09-18T11:51:51.315760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have the data in the format we wanted. Now we can construct the dataloader and train a CNN model.","metadata":{}},{"cell_type":"code","source":"filename_labels.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.319185Z","iopub.execute_input":"2023-09-18T11:51:51.325924Z","iopub.status.idle":"2023-09-18T11:51:51.341943Z","shell.execute_reply.started":"2023-09-18T11:51:51.325892Z","shell.execute_reply":"2023-09-18T11:51:51.341121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"markdown","source":"We're going to take a ResNet-50 model pretrained on ImageNet (directly available via fastai library) and fine-tune it on our small data.\n\nFirst, we construct our dataloader. We perform some data augmentation by randomly cropping images to 224x224. Setting pin memory to true enables faster data transfer between CPU and GPU, potentially speeding up training. We choose a relatively large batch size (64) to make good use of GPU.","metadata":{}},{"cell_type":"code","source":"def standardize_pixel_array(fn):\n    \"\"\"\n    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n    \"\"\"\n    # Correct DICOM pixel_array if PixelRepresentation == 1.\n    dcm = pydicom.dcmread(fn)\n    pixel_array = dcm.pixel_array\n    if dcm.PixelRepresentation == 1:\n        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n        dtype = pixel_array.dtype \n        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n#         pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n\n    intercept = float(dcm.RescaleIntercept)\n    slope = float(dcm.RescaleSlope)\n    center = int(dcm.WindowCenter)\n    width = int(dcm.WindowWidth)\n    low = center - width / 2\n    high = center + width / 2    \n    \n    pixel_array = (pixel_array * slope) + intercept\n    pixel_array = np.clip(pixel_array, low, high)\n\n    return pixel_array","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.343350Z","iopub.execute_input":"2023-09-18T11:51:51.343993Z","iopub.status.idle":"2023-09-18T11:51:51.358207Z","shell.execute_reply.started":"2023-09-18T11:51:51.343942Z","shell.execute_reply":"2023-09-18T11:51:51.357346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PILDicom2(PILBase):\n    \"same as PILDicom but changed pixel array dtype to int32 since uint16 cannot be handled by PIL/PyTorch\"\n    \n    _open_args,_tensor_cls,_show_args = {},TensorDicom,TensorDicom._show_args\n    @classmethod\n    def create(cls, fn:Path|str|bytes, mode=None)->None:\n        \"Open a `DICOM file` from path `fn` or bytes `fn` and load it as a `PIL Image`\"\n        if isinstance(fn,bytes): im = Image.fromarray(pydicom.dcmread(pydicom.filebase.DicomBytesIO(fn)).pixel_array)\n        if isinstance(fn,(Path,str)): im = Image.fromarray(standardize_pixel_array(fn).astype(np.int32))\n        im.load()\n        im = im._new(im.im)\n        return cls(im.convert(mode) if mode else im)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.360408Z","iopub.execute_input":"2023-09-18T11:51:51.361611Z","iopub.status.idle":"2023-09-18T11:51:51.376121Z","shell.execute_reply.started":"2023-09-18T11:51:51.361575Z","shell.execute_reply":"2023-09-18T11:51:51.375132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_block = DataBlock(blocks = (ImageBlock(cls=PILDicom2), MultiCategoryBlock),\n                      splitter = ColSplitter('is_valid'),\n                      get_x = ColReader('fname', pref = BASE_DIR),\n                      get_y = ColReader('labels', label_delim = ' '),\n                      item_tfms = Resize(512, resamples = (0, 0)))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.380018Z","iopub.execute_input":"2023-09-18T11:51:51.381224Z","iopub.status.idle":"2023-09-18T11:51:51.395541Z","shell.execute_reply.started":"2023-09-18T11:51:51.381194Z","shell.execute_reply":"2023-09-18T11:51:51.394698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = data_block.dataloaders(filename_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:51.400252Z","iopub.execute_input":"2023-09-18T11:51:51.402063Z","iopub.status.idle":"2023-09-18T11:51:57.009973Z","shell.execute_reply.started":"2023-09-18T11:51:51.402031Z","shell.execute_reply":"2023-09-18T11:51:57.008979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dsets = data_block.datasets(filename_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:57.011452Z","iopub.execute_input":"2023-09-18T11:51:57.011788Z","iopub.status.idle":"2023-09-18T11:51:57.063715Z","shell.execute_reply.started":"2023-09-18T11:51:57.011753Z","shell.execute_reply":"2023-09-18T11:51:57.062860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:57.064996Z","iopub.execute_input":"2023-09-18T11:51:57.065417Z","iopub.status.idle":"2023-09-18T11:51:59.816005Z","shell.execute_reply.started":"2023-09-18T11:51:57.065374Z","shell.execute_reply":"2023-09-18T11:51:59.815103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to use appropriate metrics for our multi-label classfication problem. F1 score is one of the metrics that can be used.\n\nWe create a learner.","metadata":{}},{"cell_type":"code","source":"dls.vocab","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:51:59.821243Z","iopub.execute_input":"2023-09-18T11:51:59.821923Z","iopub.status.idle":"2023-09-18T11:51:59.829221Z","shell.execute_reply.started":"2023-09-18T11:51:59.821889Z","shell.execute_reply":"2023-09-18T11:51:59.827936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\nf1_macro.name = 'F1(macro)'\nf1_samples = F1ScoreMulti(thresh=0.5, average='samples')\nf1_samples.name = 'F1(samples)'\nlearn = vision_learner(dls, resnet50, metrics=[partial(accuracy_multi, thresh=0.5), f1_macro], model_dir = '/kaggle/working/models')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T11:56:27.569154Z","iopub.execute_input":"2023-09-18T11:56:27.569758Z","iopub.status.idle":"2023-09-18T11:56:29.528054Z","shell.execute_reply.started":"2023-09-18T11:56:27.569715Z","shell.execute_reply":"2023-09-18T11:56:29.526565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:00:30.419056Z","iopub.execute_input":"2023-09-18T12:00:30.419658Z","iopub.status.idle":"2023-09-18T12:00:38.267465Z","shell.execute_reply.started":"2023-09-18T12:00:30.419614Z","shell.execute_reply":"2023-09-18T12:00:38.266532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,o in reversed(list(enumerate(learn.children()))):\n    print(i,o)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:07:27.442588Z","iopub.execute_input":"2023-09-18T12:07:27.443042Z","iopub.status.idle":"2023-09-18T12:07:27.453057Z","shell.execute_reply.started":"2023-09-18T12:07:27.443004Z","shell.execute_reply":"2023-09-18T12:07:27.451903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"head_bowel = create_head(2048, 1)\nhead_extravasation = create_head(2048, 1)\nhead_kidney = create_head(2048, 3)\nhead_liver = create_head(2048, 3)\nhead_spleen = create_head(2048, 3)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:17:33.702460Z","iopub.execute_input":"2023-09-18T12:17:33.702961Z","iopub.status.idle":"2023-09-18T12:17:33.873630Z","shell.execute_reply.started":"2023-09-18T12:17:33.702921Z","shell.execute_reply":"2023-09-18T12:17:33.872619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"body = create_body(learn)\nfull_head = torch.cat(head_bowel, head_extravasation, head_kidney, head_liver, head_spleen)\nlearn_new = nn.Sequential(body, full_head)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T12:24:16.965143Z","iopub.execute_input":"2023-09-18T12:24:16.965607Z","iopub.status.idle":"2023-09-18T12:24:17.044345Z","shell.execute_reply.started":"2023-09-18T12:24:16.965571Z","shell.execute_reply":"2023-09-18T12:24:17.041925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.opt_func","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:59:28.259617Z","iopub.execute_input":"2023-09-18T09:59:28.260114Z","iopub.status.idle":"2023-09-18T09:59:28.271717Z","shell.execute_reply.started":"2023-09-18T09:59:28.260079Z","shell.execute_reply":"2023-09-18T09:59:28.270566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do mixed-precision training to speed up our training process.","metadata":{}},{"cell_type":"code","source":"learn.to_fp16()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:59:31.629345Z","iopub.execute_input":"2023-09-18T09:59:31.629841Z","iopub.status.idle":"2023-09-18T09:59:31.638506Z","shell.execute_reply.started":"2023-09-18T09:59:31.629803Z","shell.execute_reply":"2023-09-18T09:59:31.637206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"fastai provides a handy function to let us find a good learning rate.","metadata":{}},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T09:59:37.094500Z","iopub.execute_input":"2023-09-18T09:59:37.094985Z","iopub.status.idle":"2023-09-18T10:02:41.181265Z","shell.execute_reply.started":"2023-09-18T09:59:37.094949Z","shell.execute_reply":"2023-09-18T10:02:41.180192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now fine tune our model for 10 epochs using the recommended learning rate.","metadata":{}},{"cell_type":"code","source":"learn.fine_tune(3, 2.1e-3)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:03:03.435855Z","iopub.execute_input":"2023-09-18T10:03:03.436316Z","iopub.status.idle":"2023-09-18T10:12:33.318130Z","shell.execute_reply.started":"2023-09-18T10:03:03.436267Z","shell.execute_reply":"2023-09-18T10:12:33.316977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the epochs progress, both the training and validation losses decrease. The F1 score reaches 1, meaning that both precision and recall are 1. Hence, our model is classifying all samples in training set correctly.\n\nLet's look at the predictions for some examples in the training set. For each image, the upper label list is the true set of labels; the lower label list is the predicted set of labels.","metadata":{}},{"cell_type":"code","source":"learn.show_results()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T10:17:26.097948Z","iopub.execute_input":"2023-09-18T10:17:26.098557Z","iopub.status.idle":"2023-09-18T10:17:30.522969Z","shell.execute_reply.started":"2023-09-18T10:17:26.098512Z","shell.execute_reply":"2023-09-18T10:17:30.522002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model is doing well to predict samples from the training set. We've used a very small amount of data to train, so probably the learning task is too easy.","metadata":{}},{"cell_type":"markdown","source":"### Save Model","metadata":{}},{"cell_type":"markdown","source":"We need to save our model, so that our [inference notebook](https://www.kaggle.com/code/pankajpansari/rsna-2023-atd-baseline-1-inference) can import it to make predictions on the test set.","metadata":{}},{"cell_type":"code","source":"learn.export('/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T12:26:17.806379Z","iopub.execute_input":"2023-09-11T12:26:17.807137Z","iopub.status.idle":"2023-09-11T12:26:18.305420Z","shell.execute_reply.started":"2023-09-11T12:26:17.807097Z","shell.execute_reply":"2023-09-11T12:26:18.304350Z"},"trusted":true},"execution_count":null,"outputs":[]}]}